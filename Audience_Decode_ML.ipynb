{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e51b249",
   "metadata": {},
   "source": [
    "# Audience Decode – ML Classification\n",
    "\n",
    "Predict whether a user will give a **high rating** (≥ 4) to a movie interaction.\n",
    "\n",
    "**Data Split:** Uses pre-split databases (random split):\n",
    "- `viewer_interactions_train.db` — 70% training data\n",
    "- `viewer_interactions_val.db` — 15% validation data\n",
    "- `viewer_interactions_test.db` — 15% test data\n",
    "\n",
    "**Models:**\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e936ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3  # connect to SQLite database\n",
    "from pathlib import Path  # handle filesystem paths\n",
    "\n",
    "import numpy as np  # numerical computations\n",
    "import pandas as pd  # data manipulation\n",
    "\n",
    "from sklearn.compose import ColumnTransformer  # apply different transforms to column subsets\n",
    "from sklearn.pipeline import Pipeline  # chain preprocessing and model\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder  # feature scaling and encoding\n",
    "from sklearn.impute import SimpleImputer  # handle missing values\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression  # logistic regression classifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # tree-based classifiers\n",
    "\n",
    "from sklearn.metrics import (  # evaluation metrics\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "\n",
    "# Paths to pre-split databases (70% train, 15% val, 15% test)\n",
    "TRAIN_DB_PATH = Path(\"./viewer_interactions_train.db\")\n",
    "VAL_DB_PATH = Path(\"./viewer_interactions_val.db\")\n",
    "TEST_DB_PATH = Path(\"./viewer_interactions_test.db\")\n",
    "\n",
    "assert TRAIN_DB_PATH.exists(), f\"Training DB not found at {TRAIN_DB_PATH}\"\n",
    "assert VAL_DB_PATH.exists(), f\"Validation DB not found at {VAL_DB_PATH}\"\n",
    "assert TEST_DB_PATH.exists(), f\"Test DB not found at {TEST_DB_PATH}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1246143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "  viewer_ratings: (2817500, 5)\n",
      "  user_statistics: (405158, 10)\n",
      "  movie_statistics: (12980, 11)\n",
      "\n",
      "Validation data:\n",
      "  viewer_ratings: (603750, 5)\n",
      "  user_statistics: (238836, 10)\n",
      "  movie_statistics: (4170, 11)\n",
      "\n",
      "Test data:\n",
      "  viewer_ratings: (603750, 5)\n",
      "  user_statistics: (238412, 10)\n",
      "  movie_statistics: (4180, 11)\n"
     ]
    }
   ],
   "source": [
    "def load_data_from_db(db_path):\n",
    "    \"\"\"Load viewer_ratings, user_statistics, movie_statistics from a database.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    viewer_ratings = pd.read_sql(\"SELECT * FROM viewer_ratings;\", conn)\n",
    "    user_statistics = pd.read_sql(\"SELECT * FROM user_statistics;\", conn)\n",
    "    movie_statistics = pd.read_sql(\"SELECT * FROM movie_statistics;\", conn)\n",
    "    conn.close()\n",
    "    return viewer_ratings, user_statistics, movie_statistics\n",
    "\n",
    "# Load training data\n",
    "train_ratings, train_user_stats, train_movie_stats = load_data_from_db(TRAIN_DB_PATH)\n",
    "print(\"Training data:\")\n",
    "print(f\"  viewer_ratings: {train_ratings.shape}\")\n",
    "print(f\"  user_statistics: {train_user_stats.shape}\")\n",
    "print(f\"  movie_statistics: {train_movie_stats.shape}\")\n",
    "\n",
    "# Load validation data\n",
    "val_ratings, val_user_stats, val_movie_stats = load_data_from_db(VAL_DB_PATH)\n",
    "print(\"\\nValidation data:\")\n",
    "print(f\"  viewer_ratings: {val_ratings.shape}\")\n",
    "print(f\"  user_statistics: {val_user_stats.shape}\")\n",
    "print(f\"  movie_statistics: {val_movie_stats.shape}\")\n",
    "\n",
    "# Load test data\n",
    "test_ratings, test_user_stats, test_movie_stats = load_data_from_db(TEST_DB_PATH)\n",
    "print(\"\\nTest data:\")\n",
    "print(f\"  viewer_ratings: {test_ratings.shape}\")\n",
    "print(f\"  user_statistics: {test_user_stats.shape}\")\n",
    "print(f\"  movie_statistics: {test_movie_stats.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbf477ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label distribution:\n",
      "label_high\n",
      "1    0.572452\n",
      "0    0.427548\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation label distribution:\n",
      "label_high\n",
      "1    0.573459\n",
      "0    0.426541\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test label distribution:\n",
      "label_high\n",
      "1    0.573636\n",
      "0    0.426364\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def preprocess_ratings(df):\n",
    "    \"\"\"Clean and preprocess ratings data, add target label.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"rating\"] = pd.to_numeric(df[\"rating\"], errors=\"coerce\")  # ensure rating is numeric\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")  # ensure date is datetime\n",
    "    \n",
    "    mask_valid = df[\"rating\"].between(1, 5)  # allow only ratings 1..5\n",
    "    df = df.loc[mask_valid].copy()  # keep only valid ratings\n",
    "    \n",
    "    if \"anomalous_date\" in df.columns:  # if anomalous_date exists\n",
    "        df = df[df[\"anomalous_date\"] != 1].copy()  # drop rows with anomalous_date == 1\n",
    "    \n",
    "    df = df.dropna(subset=[\"date\"]).copy()  # drop rows with missing date\n",
    "    df[\"label_high\"] = (df[\"rating\"] >= 4).astype(int)  # 1 if rating >= 4, else 0\n",
    "    return df\n",
    "\n",
    "# Preprocess all three datasets\n",
    "train_df = preprocess_ratings(train_ratings)\n",
    "val_df = preprocess_ratings(val_ratings)\n",
    "test_df = preprocess_ratings(test_ratings)\n",
    "\n",
    "print(\"Training label distribution:\")\n",
    "print(train_df[\"label_high\"].value_counts(normalize=True))\n",
    "print(f\"\\nValidation label distribution:\")\n",
    "print(val_df[\"label_high\"].value_counts(normalize=True))\n",
    "print(f\"\\nTest label distribution:\")\n",
    "print(test_df[\"label_high\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff8660ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training user stats shape: (405158, 10)\n",
      "Training movie stats shape: (12980, 11)\n"
     ]
    }
   ],
   "source": [
    "def prepare_stats(user_statistics, movie_statistics):\n",
    "    \"\"\"Rename columns in user and movie statistics for merging.\"\"\"\n",
    "    user_stats = user_statistics.copy()\n",
    "    movie_stats = movie_statistics.copy()\n",
    "    \n",
    "    user_rename = {\n",
    "        \"total_ratings\": \"user_total_ratings\",\n",
    "        \"unique_movies\": \"user_unique_movies\",\n",
    "        \"avg_rating\": \"user_avg_rating\",\n",
    "        \"std_rating\": \"user_std_rating\",\n",
    "        \"activity_days\": \"user_activity_days\",\n",
    "    }\n",
    "    user_stats = user_stats.rename(columns=user_rename)\n",
    "    \n",
    "    movie_rename = {\n",
    "        \"total_ratings\": \"movie_total_ratings\",\n",
    "        \"unique_users\": \"movie_unique_users\",\n",
    "        \"avg_rating\": \"movie_avg_rating\",\n",
    "        \"year_of_release\": \"movie_year_of_release\",\n",
    "    }\n",
    "    movie_stats = movie_stats.rename(columns=movie_rename)\n",
    "    \n",
    "    return user_stats, movie_stats\n",
    "\n",
    "# Prepare stats for each split\n",
    "train_user_stats_renamed, train_movie_stats_renamed = prepare_stats(train_user_stats, train_movie_stats)\n",
    "val_user_stats_renamed, val_movie_stats_renamed = prepare_stats(val_user_stats, val_movie_stats)\n",
    "test_user_stats_renamed, test_movie_stats_renamed = prepare_stats(test_user_stats, test_movie_stats)\n",
    "\n",
    "print(\"Training user stats shape:\", train_user_stats_renamed.shape)\n",
    "print(\"Training movie stats shape:\", train_movie_stats_renamed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a7ce95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train merged shape: (2534660, 24)\n",
      "Val merged shape: (542941, 24)\n",
      "Test merged shape: (543038, 24)\n"
     ]
    }
   ],
   "source": [
    "def merge_interactions(df, user_stats, movie_stats):\n",
    "    \"\"\"Merge ratings with user and movie statistics.\"\"\"\n",
    "    interactions = df[[\"customer_id\", \"movie_id\", \"rating\", \"date\", \"label_high\"]].copy()\n",
    "    \n",
    "    interactions = interactions.merge(\n",
    "        user_stats,\n",
    "        how=\"inner\",\n",
    "        on=\"customer_id\",\n",
    "    )\n",
    "    \n",
    "    interactions = interactions.merge(\n",
    "        movie_stats,\n",
    "        how=\"inner\",\n",
    "        on=\"movie_id\",\n",
    "    )\n",
    "    \n",
    "    return interactions\n",
    "\n",
    "# Merge all three splits\n",
    "train_interactions = merge_interactions(train_df, train_user_stats_renamed, train_movie_stats_renamed)\n",
    "val_interactions = merge_interactions(val_df, val_user_stats_renamed, val_movie_stats_renamed)\n",
    "test_interactions = merge_interactions(test_df, test_user_stats_renamed, test_movie_stats_renamed)\n",
    "\n",
    "print(\"Train merged shape:\", train_interactions.shape)\n",
    "print(\"Val merged shape:\", val_interactions.shape)\n",
    "print(\"Test merged shape:\", test_interactions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b401859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample features from training data:\n",
      "   user_total_ratings  user_activity_days  user_ratings_per_month\n",
      "0                   7                 127                1.653543\n",
      "1                  18                1113                0.485175\n",
      "2                  43                 619                2.084006\n",
      "3                   4                  15                8.000000\n",
      "4                  13                  73                5.342466\n"
     ]
    }
   ],
   "source": [
    "def add_features(interactions):\n",
    "    \"\"\"Add engineered features to interactions dataframe.\"\"\"\n",
    "    df = interactions.copy()\n",
    "    \n",
    "    df[\"rating_year\"] = df[\"date\"].dt.year  # extract rating year\n",
    "    df[\"rating_month\"] = df[\"date\"].dt.month  # extract rating month\n",
    "    df[\"rating_dayofweek\"] = df[\"date\"].dt.weekday  # extract weekday\n",
    "    \n",
    "    df[\"movie_year_of_release\"] = pd.to_numeric(\n",
    "        df[\"movie_year_of_release\"], errors=\"coerce\"\n",
    "    )\n",
    "    df[\"movie_age_at_rating\"] = df[\"rating_year\"] - df[\"movie_year_of_release\"]\n",
    "    \n",
    "    df[\"user_activity_days\"] = pd.to_numeric(\n",
    "        df[\"user_activity_days\"], errors=\"coerce\"\n",
    "    )\n",
    "    activity_days_safe = df[\"user_activity_days\"].replace(0, np.nan)\n",
    "    df[\"user_ratings_per_month\"] = df[\"user_total_ratings\"] / (activity_days_safe / 30.0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add features to all splits\n",
    "train_interactions = add_features(train_interactions)\n",
    "val_interactions = add_features(val_interactions)\n",
    "test_interactions = add_features(test_interactions)\n",
    "\n",
    "print(\"Sample features from training data:\")\n",
    "print(train_interactions[[\"user_total_ratings\", \"user_activity_days\", \"user_ratings_per_month\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb2b155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2534660, 14) y_train shape: (2534660,)\n",
      "X_val shape: (542941, 14) y_val shape: (542941,)\n",
      "X_test shape: (543038, 14) y_test shape: (543038,)\n"
     ]
    }
   ],
   "source": [
    "numeric_features = [  # numeric columns\n",
    "    \"user_total_ratings\",\n",
    "    \"user_unique_movies\",\n",
    "    \"user_avg_rating\",\n",
    "    \"user_std_rating\",\n",
    "    \"user_activity_days\",\n",
    "    \"user_ratings_per_month\",\n",
    "    \"movie_total_ratings\",\n",
    "    \"movie_unique_users\",\n",
    "    \"movie_avg_rating\",\n",
    "    \"movie_year_of_release\",\n",
    "    \"movie_age_at_rating\",\n",
    "    \"rating_year\",\n",
    "]\n",
    "\n",
    "categorical_features = [  # categorical columns\n",
    "    \"rating_month\",\n",
    "    \"rating_dayofweek\",\n",
    "]\n",
    "\n",
    "def prepare_X_y(interactions, numeric_features, categorical_features):\n",
    "    \"\"\"Prepare feature matrix X and target y from interactions.\"\"\"\n",
    "    model_df = interactions[numeric_features + categorical_features + [\"label_high\"]].copy()\n",
    "    model_df = model_df.dropna(subset=[\"label_high\"])\n",
    "    \n",
    "    X = model_df[numeric_features + categorical_features].copy()\n",
    "    y = model_df[\"label_high\"].astype(int).copy()\n",
    "    return X, y\n",
    "\n",
    "# Prepare X, y for all splits\n",
    "X_train, y_train = prepare_X_y(train_interactions, numeric_features, categorical_features)\n",
    "X_val, y_val = prepare_X_y(val_interactions, numeric_features, categorical_features)\n",
    "X_test, y_test = prepare_X_y(test_interactions, numeric_features, categorical_features)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape, \"y_val shape:\", y_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape, \"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a8d4c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Split Summary ===\n",
      "Train size: 2,534,660 (70.0%)\n",
      "Val size: 542,941 (15.0%)\n",
      "Test size: 543,038 (15.0%)\n",
      "\n",
      "Total: 3,620,639\n"
     ]
    }
   ],
   "source": [
    "# Data is already split from separate databases (70% train, 15% val, 15% test)\n",
    "# No need to use train_test_split - we use the pre-split databases\n",
    "\n",
    "print(\"=== Dataset Split Summary ===\")\n",
    "print(f\"Train size: {X_train.shape[0]:,} ({X_train.shape[0] / (X_train.shape[0] + X_val.shape[0] + X_test.shape[0]) * 100:.1f}%)\")\n",
    "print(f\"Val size: {X_val.shape[0]:,} ({X_val.shape[0] / (X_train.shape[0] + X_val.shape[0] + X_test.shape[0]) * 100:.1f}%)\")\n",
    "print(f\"Test size: {X_test.shape[0]:,} ({X_test.shape[0] / (X_train.shape[0] + X_val.shape[0] + X_test.shape[0]) * 100:.1f}%)\")\n",
    "print(f\"\\nTotal: {X_train.shape[0] + X_val.shape[0] + X_test.shape[0]:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f8d02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(  # pipeline for numeric features\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),  # fill missing values with median\n",
    "        (\"scaler\", StandardScaler()),  # standardize numeric features\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(  # pipeline for categorical features\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # fill missing with most frequent\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),  # one-hot encode categories\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(  # apply transformers to columns\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac835bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(  # logistic regression model\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    ")\n",
    "\n",
    "rf_clf = RandomForestClassifier(  # random forest classifier\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(  # gradient boosting classifier\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "models = {  # dict of model pipelines\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", log_reg),\n",
    "    ]),\n",
    "    \"Random Forest\": Pipeline([\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", rf_clf),\n",
    "    ]),\n",
    "    \"Gradient Boosting\": Pipeline([\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", gb_clf),\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ec849ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "Training: Logistic Regression\n",
      "Val Accuracy: 0.8029\n",
      "Val Precision: 0.8471\n",
      "Val Recall: 0.8008\n",
      "Val F1: 0.8233\n",
      "Val ROC-AUC: 0.8849\n",
      "Classification report (val):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.81      0.78    231586\n",
      "           1       0.85      0.80      0.82    311355\n",
      "\n",
      "    accuracy                           0.80    542941\n",
      "   macro avg       0.80      0.80      0.80    542941\n",
      "weighted avg       0.81      0.80      0.80    542941\n",
      "\n",
      "\n",
      "====================\n",
      "Training: Random Forest\n",
      "Val Accuracy: 0.8169\n",
      "Val Precision: 0.8314\n",
      "Val Recall: 0.8538\n",
      "Val F1: 0.8424\n",
      "Val ROC-AUC: 0.9130\n",
      "Classification report (val):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.78    231586\n",
      "           1       0.83      0.85      0.84    311355\n",
      "\n",
      "    accuracy                           0.82    542941\n",
      "   macro avg       0.81      0.81      0.81    542941\n",
      "weighted avg       0.82      0.82      0.82    542941\n",
      "\n",
      "\n",
      "====================\n",
      "Training: Gradient Boosting\n",
      "Val Accuracy: 0.8188\n",
      "Val Precision: 0.8260\n",
      "Val Recall: 0.8666\n",
      "Val F1: 0.8458\n",
      "Val ROC-AUC: 0.9153\n",
      "Classification report (val):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78    231586\n",
      "           1       0.83      0.87      0.85    311355\n",
      "\n",
      "    accuracy                           0.82    542941\n",
      "   macro avg       0.82      0.81      0.81    542941\n",
      "weighted avg       0.82      0.82      0.82    542941\n",
      "\n",
      "\n",
      "Validation metrics:\n",
      "                 model  accuracy  precision    recall        f1   roc_auc\n",
      "0  Logistic Regression  0.802852   0.847091  0.800758  0.823273  0.884854\n",
      "1        Random Forest  0.816864   0.831419  0.853759  0.842441  0.912982\n",
      "2    Gradient Boosting  0.818818   0.826035  0.866554  0.845809  0.915283\n"
     ]
    }
   ],
   "source": [
    "val_results = []  # list to store validation metrics\n",
    "\n",
    "for name, pipe in models.items():  # loop over models\n",
    "    print(\"\\n====================\")  # separator\n",
    "    print(\"Training:\", name)  # show model name\n",
    "    pipe.fit(X_train, y_train)  # fit model on train set\n",
    "\n",
    "    y_val_pred = pipe.predict(X_val)  # predict labels on val set\n",
    "    y_val_proba = pipe.predict_proba(X_val)[:, 1]  # predict proba for class 1\n",
    "\n",
    "    acc = accuracy_score(y_val, y_val_pred)  # accuracy\n",
    "    prec = precision_score(y_val, y_val_pred, zero_division=0)  # precision\n",
    "    rec = recall_score(y_val, y_val_pred, zero_division=0)  # recall\n",
    "    f1 = f1_score(y_val, y_val_pred, zero_division=0)  # F1-score\n",
    "    roc_auc = roc_auc_score(y_val, y_val_proba)  # ROC-AUC\n",
    "\n",
    "    print(f\"Val Accuracy: {acc:.4f}\")  # show accuracy\n",
    "    print(f\"Val Precision: {prec:.4f}\")  # show precision\n",
    "    print(f\"Val Recall: {rec:.4f}\")  # show recall\n",
    "    print(f\"Val F1: {f1:.4f}\")  # show F1\n",
    "    print(f\"Val ROC-AUC: {roc_auc:.4f}\")  # show ROC-AUC\n",
    "\n",
    "    print(\"Classification report (val):\")  # header\n",
    "    print(classification_report(y_val, y_val_pred, zero_division=0))  # detailed report\n",
    "\n",
    "    val_results.append({  # store metrics\n",
    "        \"model\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "    })\n",
    "\n",
    "val_results_df = pd.DataFrame(val_results)  # metrics table\n",
    "print(\"\\nValidation metrics:\")  # header\n",
    "print(val_results_df)  # show metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b896561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Gradient Boosting\n",
      "\n",
      "=== Test performance ===\n",
      "Test Accuracy: 0.8181\n",
      "Test Precision: 0.8299\n",
      "Test Recall: 0.8589\n",
      "Test F1: 0.8441\n",
      "Test ROC-AUC: 0.9147\n",
      "\n",
      "Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78    231532\n",
      "           1       0.83      0.86      0.84    311506\n",
      "\n",
      "    accuracy                           0.82    543038\n",
      "   macro avg       0.82      0.81      0.81    543038\n",
      "weighted avg       0.82      0.82      0.82    543038\n",
      "\n",
      "Confusion matrix (test):\n",
      "[[176676  54856]\n",
      " [ 43949 267557]]\n"
     ]
    }
   ],
   "source": [
    "best_idx = val_results_df[\"f1\"].idxmax()  # index of best model by F1\n",
    "best_name = val_results_df.loc[best_idx, \"model\"]  # name of best model\n",
    "print(\"Best model:\", best_name)  # show best model\n",
    "\n",
    "best_pipe = models[best_name]  # corresponding pipeline\n",
    "\n",
    "X_train_full = pd.concat([X_train, X_val], axis=0)  # merge train and val features\n",
    "y_train_full = pd.concat([y_train, y_val], axis=0)  # merge train and val labels\n",
    "\n",
    "best_pipe.fit(X_train_full, y_train_full)  # refit on full train data\n",
    "\n",
    "y_test_pred = best_pipe.predict(X_test)  # predict labels on test set\n",
    "y_test_proba = best_pipe.predict_proba(X_test)[:, 1]  # predict proba on test set\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_test_pred)  # test accuracy\n",
    "test_prec = precision_score(y_test, y_test_pred, zero_division=0)  # test precision\n",
    "test_rec = recall_score(y_test, y_test_pred, zero_division=0)  # test recall\n",
    "test_f1 = f1_score(y_test, y_test_pred, zero_division=0)  # test F1\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_proba)  # test ROC-AUC\n",
    "\n",
    "print(\"\\n=== Test performance ===\")  # header\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")  # show accuracy\n",
    "print(f\"Test Precision: {test_prec:.4f}\")  # show precision\n",
    "print(f\"Test Recall: {test_rec:.4f}\")  # show recall\n",
    "print(f\"Test F1: {test_f1:.4f}\")  # show F1\n",
    "print(f\"Test ROC-AUC: {test_roc_auc:.4f}\")  # show ROC-AUC\n",
    "\n",
    "print(\"\\nClassification report (test):\")  # header\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))  # detailed report\n",
    "\n",
    "print(\"Confusion matrix (test):\")  # header\n",
    "print(confusion_matrix(y_test, y_test_pred))  # confusion matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
